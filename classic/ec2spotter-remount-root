#!/bin/bash

# "Phase 2" this script is invoked inside of the spot VM at boot time to find the root volume.

if ! [ "x$1" = "x--force" ]
then 
  # this is here just to cover accidental invocation from the command-line. it should "never happen"
  echo "This script is destructive when invoked at the wrong time. If you are seeing this message you are doing something wrong."
  exit -1
fi

if [ "x$2" = "x" ]; then echo "missing volume name"; exit -1; fi

ROOT_VOL_NAME=$2
ec2spotter_elastic_ip=$3

export INSTANCE_ID=`curl -s http://169.254.169.254/latest/meta-data/instance-id`
export SPOT_ZONE=`curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone`
export SPOT_REGION=${SPOT_ZONE::-1} # Get the region by stripping the last character of the ROOT_ZONE
export AWS_DEFAULT_REGION=$SPOT_REGION
ROOT_REGION=$SPOT_REGION # By default, we're in the same region
#ROOT_ZONE=$SPOT_ZONE

echo "Looking for a volume in $ROOT_REGION with Name = $ROOT_VOL_NAME"

aws ec2 describe-volumes \
        --filters Name=tag-key,Values="Name" Name=tag-value,Values="$ROOT_VOL_NAME" \
        --region ${SPOT_REGION} > volumes.tmp || exit -1

ROOT_ZONE=$(jq -r '.Volumes[0].AvailabilityZone' volumes.tmp)
ROOT_VOL=$(jq -r '.Volumes[0].VolumeId' volumes.tmp)
ROOT_TYPE=$(jq -r '.Volumes[0].VolumeType' volumes.tmp)

# Are we copying the volume from the same region?
if [[ $ROOT_ZONE != $SPOT_ZONE ]]; then
    # need to copy the volume across
    echo "Volume $ROOT_VOL is in another Availability Zone"
    echo "Creating a snapshot of the volume"
    SNAPSHOT=$(aws ec2 create-snapshot --region ${ROOT_REGION} --volume-id $ROOT_VOL --description 'ec2-spotter temporary snapshot ok to delete' | jq -r ".SnapshotId")
    
    echo "Snapshot $SNAPSHOT created. Waiting for completion"
    # Keep checking to see that snapshot has been created
    count=0
    while /bin/true
    do
        sleep 5 
        eval count=$((count+30))
        echo "... $count seconds gone. Still waiting..."
        STATUS=$(aws ec2 describe-snapshots --region ${ROOT_REGION} --snapshot-ids ${SNAPSHOT} | grep completed)

        [[ ! -z $STATUS ]] && break
    done
    echo "Snapshot $SNAPSHOT created successfully"
    echo "------------------------------------------------"
    echo ""
    
    NEW_SNAPSHOT=$SNAPSHOT

    # create volume from this new snapshot
    TAG_SPEC='ResourceType=volume,Tags=[{Key=Name,Value=' 
    TAG_SPEC+="${ROOT_VOL_NAME}"
    TAG_SPEC+='}]'
    NEW_VOLUME=$(aws ec2 create-volume --region ${ROOT_REGION} --snapshot-id ${NEW_SNAPSHOT} --availability-zone ${SPOT_ZONE} --volume-type $ROOT_TYPE --tag-specifications "${TAG_SPEC}" | jq -r '.VolumeId')
    echo "Creating volume $NEW_VOLUME from $NEW_SNAPSHOT. Waiting for completion"
    
    # Keep checking to see that volume has been created
    count=0
    while /bin/true
    do
        sleep 5 
        eval count=$((count+30))
        echo "... $count seconds gone. Still waiting..."
        STATUS=$(aws ec2 describe-volumes --volume-ids ${NEW_VOLUME} | grep available)

        [[ ! -z $STATUS ]] && break
    done
    
    echo "Volume $NEW_VOLUME created successfully"
    echo "------------------------------------------------"

    echo "Deleting the old volume"
    aws ec2 delete-volume  --volume-id $ROOT_VOL --region ${SPOT_REGION}

    PERSISTENT_ROOT_VOLUME=${NEW_VOLUME}
else
    echo "Using the volume as-is, no migration needed"
    PERSISTENT_ROOT_VOLUME=${ROOT_VOL}
fi

echo ""
echo "Attaching volume $PERSISTENT_ROOT_VOLUME as /dev/sdf"
# Attach volume
aws ec2 attach-volume --volume-id $PERSISTENT_ROOT_VOLUME --instance-id $INSTANCE_ID --device /dev/sdf
# ${APIBIN}/ec2-attach-volume $PERSISTENT_ROOT_VOLUME -d /dev/sdf --instance $INSTANCE_ID || exit -1

DEVICE=/dev/nvme1n1p1
while ! lsblk $DEVICE 
do
  echo "waiting for device to attach"
  sleep 10
done

# acquire the elastic ip
if ! [ "$ec2spotter_elastic_ip"x = x ]
then
  aws ec2 associate-address --instance-id $INSTANCE_ID --allocation-id $ec2spotter_elastic_ip --allow-reassociation --region ${SPOT_REGION}
fi

NEWMNT=/permaroot
OLDMNT=old-root
mkdir -p $NEWMNT
mount -o nouuid $DEVICE /permaroot
umount $DEVICE
xfs_repair $DEVICE


mv /usr/lib/systemd/systemd /usr/lib/systemd/systemd.real
cat >/usr/lib/systemd/systemd <<EOF
#!/bin/sh

if [ "x\$1" == "x--switched-root" ]; then

  mkdir -p /permaroot
  mount /dev/nvme1n1p1 /permaroot
  echo Remounted permaroot with exit code \$? > /dev/kmsg

  cd /permaroot
  unshare -m

  for dir in /dev /proc /sys /run; do
    echo "Moving mounted file system \${dir} to .\$dir." > /dev/kmsg
    mount --move \${dir} .\${dir}
  done

  echo "About to chroot in 5 secs..."
  sleep 5
  exec /sbin/chroot . /usr/lib/systemd/systemd \$@
  
else
  exec /usr/lib/systemd/systemd.real \$@
fi
EOF
chmod +x /usr/lib/systemd/systemd

cat >/tmp/cloud-init-wait.sh <<EOF
#!/bin/sh
INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
(while ! test -d "/var/lib/cloud/instances/$INSTANCE_ID"; do
  echo "Waiting for cloud init to finish"
  sleep 10
done
echo "Rebooting in 5 seconds to get to chroot"
sleep 5
#reboot
echo "Would have rebooted") > /dev/kmsg
EOF

nohup bash /tmp/cloud-init-wait.sh &
