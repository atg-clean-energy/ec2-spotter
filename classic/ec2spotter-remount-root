#!/bin/bash

# "Phase 2" this script is invoked inside of the spot VM at boot time to find the root volume.

if ! [ "x$1" = "x--force" ]
then 
  # this is here just to cover accidental invocation from the command-line. it should "never happen"
  echo "This script is destructive when invoked at the wrong time. If you are seeing this message you are doing something wrong."
  exit -1
fi

if [ "x$2" = "x" ]; then echo "missing volume name"; exit -1; fi

ROOT_VOL_NAME=$2
ec2spotter_elastic_ip=$3

export INSTANCE_ID=`curl -s http://169.254.169.254/latest/meta-data/instance-id`
export SPOT_ZONE=`curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone`
export SPOT_REGION=${SPOT_ZONE::-1} # Get the region by stripping the last character of the ROOT_ZONE
export AWS_DEFAULT_REGION=$SPOT_REGION
ROOT_REGION=$SPOT_REGION # By default, we're in the same region
#ROOT_ZONE=$SPOT_ZONE

echo "Looking for a volume in $ROOT_REGION with Name = $ROOT_VOL_NAME"

aws ec2 describe-volumes \
        --filters Name=tag-key,Values="Name" Name=tag-value,Values="$ROOT_VOL_NAME" \
        --region ${SPOT_REGION} > volumes.tmp || exit -1

ROOT_ZONE=$(jq -r '.Volumes[0].AvailabilityZone' volumes.tmp)
ROOT_VOL=$(jq -r '.Volumes[0].VolumeId' volumes.tmp)
ROOT_TYPE=$(jq -r '.Volumes[0].VolumeType' volumes.tmp)

# Are we copying the volume from the same region?
if [[ $ROOT_ZONE != $SPOT_ZONE ]]; then
    # need to copy the volume across
    echo "Volume $ROOT_VOL is in another Availability Zone"
    echo "Creating a snapshot of the volume"
    SNAPSHOT=$(aws ec2 create-snapshot --region ${ROOT_REGION} --volume-id $ROOT_VOL --description 'ec2-spotter temporary snapshot ok to delete' | jq -r ".SnapshotId")
    
    echo "Snapshot $SNAPSHOT created. Waiting for completion"
    # Keep checking to see that snapshot has been created
    count=0
    while /bin/true
    do
        sleep 5 
        eval count=$((count+30))
        echo "... $count seconds gone. Still waiting..."
        STATUS=$(aws ec2 describe-snapshots --region ${ROOT_REGION} --snapshot-ids ${SNAPSHOT} | grep completed)

        [[ ! -z $STATUS ]] && break
    done
    echo "Snapshot $SNAPSHOT created successfully"
    echo "------------------------------------------------"
    echo ""
    
    NEW_SNAPSHOT=$SNAPSHOT

    # create volume from this new snapshot
    TAG_SPEC='ResourceType=volume,Tags=[{Key=Name,Value=' 
    TAG_SPEC+="${ROOT_VOL_NAME}"
    TAG_SPEC+='}]'
    NEW_VOLUME=$(aws ec2 create-volume --region ${ROOT_REGION} --snapshot-id ${NEW_SNAPSHOT} --availability-zone ${SPOT_ZONE} --volume-type $ROOT_TYPE --tag-specifications "${TAG_SPEC}" | jq -r '.VolumeId')
    echo "Creating volume $NEW_VOLUME from $NEW_SNAPSHOT. Waiting for completion"
    
    # Keep checking to see that volume has been created
    count=0
    while /bin/true
    do
        sleep 5 
        eval count=$((count+30))
        echo "... $count seconds gone. Still waiting..."
        STATUS=$(aws ec2 describe-volumes --volume-ids ${NEW_VOLUME} | grep available)

        [[ ! -z $STATUS ]] && break
    done
    
    echo "Volume $NEW_VOLUME created successfully"
    echo "------------------------------------------------"

    echo "Deleting the old volume"
    aws ec2 delete-volume  --volume-id $ROOT_VOL --region ${SPOT_REGION}

    PERSISTENT_ROOT_VOLUME=${NEW_VOLUME}
else
    echo "Using the volume as-is, no migration needed"
    PERSISTENT_ROOT_VOLUME=${ROOT_VOL}
fi

echo ""
echo "Attaching volume $PERSISTENT_ROOT_VOLUME as /dev/sdf"
# Attach volume
aws ec2 attach-volume --volume-id $PERSISTENT_ROOT_VOLUME --instance-id $INSTANCE_ID --device /dev/sdf
# ${APIBIN}/ec2-attach-volume $PERSISTENT_ROOT_VOLUME -d /dev/sdf --instance $INSTANCE_ID || exit -1

DEVICE=/dev/nvme1n1p1
while ! lsblk $DEVICE 
do
  echo "waiting for device to attach"
  sleep 10
done

# acquire the elastic ip
if ! [ "$ec2spotter_elastic_ip"x = x ]
then
  aws ec2 associate-address --instance-id $INSTANCE_ID --allocation-id $ec2spotter_elastic_ip --allow-reassociation --region ${SPOT_REGION}
fi

NEWMNT=/permaroot
OLDMNT=old-root
mkdir -p $NEWMNT
mount $DEVICE /permaroot
umount $DEVICE
xfs_repair $DEVICE

xfs_admin -L \x2f $DEVICE
xfs_admin -L oldroot /dev/nvme0n1p1

#
# point of no return... 
# modify /sbin/init on the ephemeral volume to chain-load from the persistent EBS volume, and then reboot.
#
#mv /sbin/init /sbin/init.backup
#cat >/sbin/init <<EOF
##!/bin/sh
#mount $DEVICE $NEWMNT
#[ ! -d $NEWMNT/$OLDMNT ] && mkdir -p $NEWMNT/$OLDMNT
#   
#cd $NEWMNT
#unshare -m 
#pivot_root . ./$OLDMNT
#   
#for dir in /dev /proc /sys /run; do
#   echo "Moving mounted file system ${OLDMNT}\${dir} to \$dir."
#   mount --move ./${OLDMNT}\${dir} \${dir}
#done
#exec chroot . /sbin/init
#EOF
#chmod +x /sbin/init
#shutdown -r now
